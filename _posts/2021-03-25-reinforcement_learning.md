---
layout:     post
title:      "强化学习学习笔记"
autor:      "dusmart"
tags:
    - notes
---

> 听说过 Alpha Go 和各种各样的玩游戏的算法，强化学习是绕不开的一环，因此准备学习下基本概念，跑一个 demo。 强化学习（英语：Reinforcement learning，简称RL）是机器学习中的一个领域，强调如何基于环境而行动，以取得最大化的预期利益。

<!--more-->

|            | 主要目标                                           | 典型应用         |                                                              |
| ---------- | -------------------------------------------------- | ---------------- | ------------------------------------------------------------ |
| 监督学习   | 给定数据，预测标签                                 | 验证码识别       | ![img](/assets/img/2021-03-25/a3c23194-cabb-4892-afa0-142fe09624fb.png) |
| 无监督学习 | 给定数据，寻找隐藏的结构                           | 无标注异常检测   | ![img](/assets/img/2021-03-25/ae8c01fc-0cc2-4c44-807e-0c18d21cf3d1.png) |
| 强化学习   | 给定环境，学习如何选择一系列行动，以最大化长期收益 | AlphaGo 多轮对话 | ![img](/assets/img/2021-03-25/a0cc9ce6-6b5e-4c42-bfe6-617351378691.png) |



# 强化学习基础

## 基本概念

![img](/assets/img/2021-03-25/80bff6dc-cbbe-4f0d-87d5-8fe77923357d.png)

**智能体**通过与**环境**不断交互，通过获得的**反馈**来不断调整自己的**策略**，最终得到该环境下的较优策略。

其中**环境**在任意时刻可以被建模为一个确定的**状态**，该状态对智能体可能是部分可见的，但是其本身是完全确定的；在该状态下对环境中的智能体采取某种动作后，环境会发生更新，更新后会获得新的状态。存在结束状态，到达**结束状态**后智能体不可以再进行交互。

其中**反馈**是指智能体得到的激励，是强化学习非常关键的一环，好的反馈往往是最重要的因素，智能体会通过这个反馈来调整自己的策略。比如在玩推箱子游戏时，这个反馈可能是将箱子推到指定位置后进行鼓励，将箱子推离指定位置后进行批判。

> 考虑一个贪吃蛇的游戏，有人给定的反馈为初始分数为5，吃一个加10，没吃到白走一步则减1，撞墙结束不加分。结果训练出了内卷蛇：直接撞墙

其中**动作**是指智能体在某个环境状态下的一次决策，该决策是由智能体当前策略决定的，该动作实施后会影响当前的环境状态。强化学习策略都是将“探索”和“利用”结合起来，探索 VS 利用，这是强化学习中至关重要的话题。

**探索**是指智能体抛弃当前学习到的知识，随机地进行一次决策，以获得一次未知动作的反馈，探索更优策略。**利用**是指智能体利用当前学习到的知识进行决策，以获得当前自认为最优的结果，从而可以使整个决策过程的长期价值更高，更有可能获得更优的反馈。

## 基本算法 Q learning

> 以一个简单的例子来了解 Q learning

| 环境   | 图图的作业是否做完                           |
| ------ | -------------------------------------------- |
| 智能体 | 图图                                         |
| 动作   | 看电视 / 写作业                              |
| 反馈   | 爸爸妈妈给的鼓励或惩罚，电视或作业带来的刺激 |

#### 图图的脑袋瓜 Q Table

图图被设定为采用Q learning算法进行自己的行为决策

那么图图的脑袋中会初始化一个 Q Table，刚开始的时候图图还一无所知，所以 Q table 为空

假设学习率 ⍺ = 1，即对当前回合的反馈全盘接受，不考虑历史的经验

假设一轮交互中未来的重要性 γ = 0，即不考虑未来，只考虑当下的刺激

| 状态\动作      | a1（看电视） | a2（做作业） |
| -------------- | ------------ | ------------ |
| S1（作业完成） | 0            | 0            |
| S2（作业空白） | 0            | 0            |

#### 图图的探索

----------

某一天图图没有作业（等价于写完了作业），开始了探索：看电视

爸爸妈妈看到之后没有表扬也没有惩罚，电视里的大耳朵图图逗笑了图图，所以图图得到反馈是 1

| 状态\动作      | a1（看电视） | a2（做作业） |
| -------------- | ------------ | ------------ |
| S1（作业完成） | 1            | 0            |
| S2（作业空白） | 0            | 0            |

----------

某一天图图没写完了作业，开始了探索：看电视

爸爸妈妈看到之后狠狠地教育了一番，虽然电视很好看，但图图非常难过，图图得到反馈-10

| 状态\动作      | a1（看电视） | a2（做作业） |
| -------------- | ------------ | ------------ |
| S1（作业完成） | 1            | 0            |
| S2（作业空白） | -10          | 0            |

------------

某一天图图没写完作业，开始了探索：写作业

爸爸妈妈看到之后觉得这才是正常的，没有鼓励，也没有批判，做作业很苦，图图得到反馈 -1

| 状态\动作      | a1（看电视） | a2（做作业） |
| -------------- | ------------ | ------------ |
| S1（作业完成） | 1            | 0            |
| S2（作业空白） | -10          | -1           |

----------

某一天图图写完了作业，开始了探索：再写一份

爸爸妈妈看到之后表扬了一番，但是做作业很苦，所以图图的反馈中和后得到了 0

| 状态\动作      | a1（看电视） | a2（做作业） |
| -------------- | ------------ | ------------ |
| S1（作业完成） | 1            | 0            |
| S2（作业空白） | -10          | -1           |

--------

#### 图图的利用

---------

某天图图没写完作业，不探索了，准备利用已有的经验做决策，他查了查表，发现没有做完作业的情况下做作业要比看电视来的好一些，因此选择了：做作业

这天爸爸妈妈心情很好，看到图图主动做作业了，两个人又大力夸赞了一番，图图中和了做作业的苦，得到了反馈1，于是更新了表格

| 状态\动作      | a1（看电视） | a2（做作业） |
| -------------- | ------------ | ------------ |
| S1（作业完成） | 1            | 0            |
| S2（作业空白） | -10          | 1            |

图图继续利用已有经验，发现作业做完了，应该去看电视，看电视得到开心，本轮反馈为1，加上之前的反馈，今天的心情为 2，是更新了表格

| 状态\动作      | a1（看电视） | a2（做作业） |
| -------------- | ------------ | ------------ |
| S1（作业完成） | 2            | 0            |
| S2（作业空白） | -10          | 1            |

--------

#### 图图的最佳决策

1. 没有作业的时候直接无限看电视，直到当天结束
2. 有作业的时候先做作业，再无限看电视，直到当天结束

#### Q learning 算法总结

##### Q learning 算法伪代码

要求环境状态可枚举，由向量 s 表示状态

要求动作空间可枚举，由向量 a 表示动作

```
Initialize Q(s,a) arbitrarily

Repeat (for each episode):
    Initialize s                                // 初始化本轮环境
    Repeat (for each step of episode):          // 回合制
        Choose a from s using policy derived from Q (e.g.,ⲉ-greedy) // 根据已有策略进行利用，或者随机进行探索
        Take action a , abserve r , s'          // 采取了某个动作，获得了反馈 r，也观察到了新的状态
        Q(s, a) += ⍺ * [r + γ*max(Q(s', ?)) - Q(s, a)]   // 核心规则
        s = s'                                  // 更新环境，继续运行
    until s is terminal
```

##### Q learning 更新规则

更新规则 `Q(s, a) += ⍺ * [r + γ*max(Q(s', ?)) - Q(s, a)]` 

- 对本轮刺激的学习率为 ⍺，中括号中的内容为学习到的 diff ；当前学习固然重要，历史的经验也很重要。
- 对一轮交互中未来收益的看重程度为 γ，max(Q(s', ?)) 表示下个状态下采取任意动作后的最优结果，即当前认为的未来最可能得到的最大收益；如果反馈的延迟性很高，未来收益很重要，那么可以将 γ 设置为 1。

##### Q learning 的探索与利用

- ⲉ-greedy 算法

指的是以 ⲉ 的概率来随机探索（在动作空间中随机选择），以 1-ⲉ 的概率利用当前结果

还有许多其他的算法可以采用，例如各种 ⲉ-greedy 算法带衰减（训练中 ⲉ 逐渐减小）的变种

## 深度强化学习 Deep Q Learning

#### 引入神经网络

现实生活中很多环境状态和动作状态都是连续或枚举空间极大的，这时图图的脑瓜就不够用了，因此需要和深度学习结合起来。类比 [验证码识别](/assets/material/2021-03-25-captcha.key) 中的不分割端到端验证码识别，当枚举空间较大时，通过向量或者矩阵来表示状态和动作会是一个比较自然的选择。

那么这里的深度神经网络就代替了原来的 Q Table，行使着同样的职责 `Q(S, a) = r`，在状态 S 和动作输入 a 的作用下得到了新的状态 S' 和反馈输出 r。

#### 更新神经网络

![img](/assets/img/2021-03-25/5739213a-7a12-44d0-bcd3-8ba2b6c63476.png)



和之前的更新规则一样，仍旧是基于观察到的反馈，结合历史经验、结合本轮未来的收益对网络进行更新。

另外，DQN 不同于 Q 的一点在于，神经网络非常适合并行运算，因此一次性通过之前积累的大批量的 (S, a, r) 进行网络更新会更有效率。这也是所谓的 off-policy 离线学习法。

![img](/assets/img/2021-03-25/45395a56-5a8a-47c5-9000-193073f6d6f7.png)



注意这里会有两个网络 Q 和 Q'，其中 Q' 是为了暂时固定住参数，该 Q' 被用来预测未来收益；

外层循环决定了训练轮数，内循环是每一轮训练中的不同时刻，不同时刻会有各自的状态

在内层循环中：

- 首先会进行“探索还是利用”的选择，然后执行这次行动，将行动结果缓存起来；
- 随机从缓存结果中采样一些训练集，通过最小化预测值和缓存中的真实值来回馈网络；
- 一段时间后及时更新 Q'

#### 几个问题

为啥要把Q'固定下来呢？

https://ai.stackexchange.com/questions/6982/why-does-dqn-require-two-different-networks

1. 节省计算时间
2. 防止差数据导致网络快速收敛到坏的结果中

# 强化学习例子 2048

> 对于我们调参工程师来说，不需要关心网络结构，也不需要关心各种策略，最重要的是将问题建模到当前的强化学习框架中，再根据我们的经验乱选几个网络对比对比、稍微调调参数。

| 环境状态 | 4*4 的 int 格子，离散空间      |
| -------- | ------------------------------ |
| 智能体   | 玩家                           |
| 动作     | 上 下 左 右，离散动作          |
| 反馈     | 每次动作都会得到一个增长的分数 |

#### 环境建模

最直观的部分是设置可见状态空间、动作空间

Board 中可以直接存储 1/2/4/8/16/32，也可以只存储他们的对数，避免入参区别过大，导致难以计算

```
Class Env():
    def __init__(self):
        # 初始化状态 board，设置动作空间
        self.board = [[]]
        self.action_space = []
        self.observation_space = self.board # 可见状态和整体状态一致
        self.reset()
    def is_done(self):
        # 是否棋盘已满，且无论如何动作都无法合并
        return is_done
    def step(self, action):
        # 根据指令移动，计算得分
        # 随机布置新的 2/4 到棋盘中
        return new_board, step_reward, self.is_done()
    def reset(self):
        # board 重置，布置若干 2/4
```

#### 反馈函数

动作情况分类

1. 执行了一步，棋盘有调整，但是没有合并
2. 执行了一步，棋盘未调整（非法操作）
3. 执行了一步，棋盘有合并

几种反馈设计

- 每执行一步加1，有合并时再加被合并的数：最终结果为在无法上移时智能体一直采用非法动作
- 每执行一步，如果棋盘未改变，直接结束游戏，有合并时加上被合并的数：智能体很容易暴毙，一不小心在初期就死亡了
- 每执行一步，如果棋盘未改变，则累计未改变次数，次数达到门限时直接结束，有合并时加上被合并的数：目前运行结果较好

#### 训练/展示过程

```
env = Env()
model = DQN(MlpPolicy, env, learning_rate=1e-4, prioritized_replay=True, verbose=1, buffer_size=50000)
model.learn(total_timesteps=int(4e5))
model.save("xxxxx")
```

```
model = DQN.load("xxxxx")
obs = env.reset()
env.render()
for i in range(1000):
    print("mask", env.mask())                # mask 为合法动作空间
    action, _states = model.predict(obs)     # 预测每个动作的价值
    prob = model.action_probability(obs)
    print("prob", prob, "mask", env.mask())
    for i, mask in enumerate(env.mask()):
        prob[i] *= mask
    action = np.argmax(prob)                 # 采用合法空间中价值较大的一个
    obs, rewards, dones, _ = env.step(action)# 执行
    total += rewards                         # 分数累计
    print(Base2048Env.ACTION_STRING[action], "rewards:", rewards, "isDone?", dones, "total_rewards", total)
    env.render()                             # 渲染环境
    if dones and rewards <= 0:
        break
```

#### 可执行文件 

环境定义、模型训练、最优模型、随机动作对比

当前模型得分1000~1400之间，最高拼出 256；随机模型得分50~300，最高拼出 64

[2048环境的建模](/assets/material/2021-03-25-env.py)

[模型训练+使用](/assets/material/2021-03-25-model.py)

[训练得到的模型](/assets/material/2021-03-25-dqn3.zip)

[随机玩2048的对比代码](/assets/material/2021-03-25-random.py)



# 参考文档

|                    |                                                              |
| ------------------ | ------------------------------------------------------------ |
| 强化学习Wiki       | https://zh.wikipedia.org/wiki/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0 |
| 框架StableBaseline | https://stable-baselines.readthedocs.io/en/master/           |
| 探索还是利用？     | https://aijishu.com/a/1060000000134022                       |
| 莫烦Python         | https://mofanpy.com/tutorials/machine-learning/reinforcement-learning/intro-DQN/ |

